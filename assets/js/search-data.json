{
  
    
        "post0": {
            "title": "Introducing fastpages",
            "content": ". We are very pleased to announce the immediate availability of fastpages. fastpages is a platform which allows you to create and host a blog for free, with no ads and many useful features, such as: . Create posts containing code, outputs of code (which can be interactive), formatted text, etc directly from Jupyter Notebooks; for instance see this great example post from Scott Hawley. Notebook posts support features such as: Interactive visualizations made with Altair remain interactive. | Hide or show cell input and output. | Collapsable code cells that are either open or closed by default. | Define the Title, Summary and other metadata via a special markdown cells | Ability to add links to Colab and GitHub automatically. | . | Create posts, including formatting and images, directly from Microsoft Word documents. | Create and edit Markdown posts entirely online using GitHub&#39;s built-in markdown editor. | Embed Twitter cards and YouTube videos. | Categorization of blog posts by user-supplied tags for discoverability. | ... and much more | . fastpages relies on Github pages for hosting, and Github Actions to automate the creation of your blog. The setup takes around three minutes, and does not require any technical knowledge or expertise. Due to built-in automation of fastpages, you don&#39;t have to fuss with conversion scripts. All you have to do is save your Jupyter notebook, Word document or markdown file into a specified directory and the rest happens automatically. Infact, this blog post is written in a Jupyter notebook, which you can see with the &quot;View on GitHub&quot; link above. . fast.ai have previously released a similar project called fast_template, which is even easier to set up, but does not support automatic creation of posts from Microsoft Word or Jupyter notebooks, including many of the features outlined above. . Because fastpages is more flexible and extensible, we recommend using it where possible. fast_template may be a better option for getting folks blogging who have no technical expertise at all, and will only be creating posts using Github&#39;s integrated online editor. . Setting Up Fastpages . The setup process of fastpages is automated with GitHub Actions, too! Upon creating a repo from the fastpages template, a pull request will automatically be opened (after ~ 30 seconds) configuring your blog so it can start working. The automated pull request will greet you with instructions like this: . . All you have to do is follow these instructions (in the PR you receive) and your new blogging site will be up and running! . Jupyter Notebooks &amp; Fastpages . In this post, we will cover special features that fastpages provides for Jupyter notebooks. You can also write your blog posts with Word documents or markdown in fastpages, which contain many, but not all the same features. . Options via FrontMatter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . All of the above settings are enabled in this post, so you can see what they look like! . the summary field (preceeded by &gt;) will be displayed under your title, and will also be used by social media to display as the description of your page. | toc: setting this to true will automatically generate a table of contents | badges: setting this to true will display Google Colab and GitHub links on your blog post. | comments: setting this to true will enable comments. See these instructions for more details. | author this will display the authors names. | categories will allow your post to be categorized on a &quot;Tags&quot; page, where readers can browse your post by categories. | . Markdown front matter is formatted similarly to notebooks. The differences between the two can be viewed on the fastpages README. . Code Folding . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . If you want to completely hide cells (not just collapse them), read these instructions. . Interactive Charts With Altair . Interactive visualizations made with Altair remain interactive! . We leave this below cell unhidden so you can enjoy a preview of syntax highlighting in fastpages, which uses the Dracula theme. . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;IMDB_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget IMDB_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | 6.1 | . 1 First Love, Last Rites | 10876.0 | 300000.0 | 6.9 | . 2 I Married a Strange Person | 203134.0 | 250000.0 | 6.8 | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | NaN | . 4 Slam | 1087521.0 | 1000000.0 | 3.4 | . Other Features . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Images w/Captions . You can include markdown images with captions like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Of course, the caption is optional. . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . More Examples . This tutorial contains more examples of what you can do with notebooks. . How fastpages Converts Notebooks to Blog Posts . fastpages uses nbdev to power the conversion process of Jupyter Notebooks to blog posts. When you save a notebook into the /_notebooks folder of your repository, GitHub Actions applies nbdev against those notebooks automatically. The same process occurs when you save Word documents or markdown files into the _word or _posts directory, respectively. . We will discuss how GitHub Actions work in a follow up blog post. . Resources &amp; Next Steps . We highly encourage you to start blogging with fastpages! Some resources that may be helpful: . fastpages repo - this is where you can go to create your own fastpages blog! | Fastai forums - nbdev &amp; blogging category. You can ask questions about fastpages here, as well as suggest new features. | nbdev: this project powers the conversion of Jupyter notebooks to blog posts. | . If you end up writing a blog post using fastpages, please let us know on Twitter: @jeremyphoward, @HamelHusain. .",
            "url": "https://adockhorn.github.io/Boosting/fastpages/jupyter/2021/03/19/introducing-fastpages.html",
            "relUrl": "/fastpages/jupyter/2021/03/19/introducing-fastpages.html",
            "date": " • Mar 19, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Formelverzeichnis Boosting",
            "content": "Formelverzeichnis . Nachfolgend finden Sie ein ausführliches Formelverzeichnis und eine Übersicht der besprochenen Herleitungen: . Formelverzeichnis Boosting . Ein vergleichbares Formelverzeichnis habe ich bereits während meiner Lehre an der Otto-von-Guericke Universität in Magdeburg erstellt. Begleitend zu meiner Lehrvertretung der Vorlesung “Computational Intelligence in Games” entstand dieses in Zusammenarbeit mit Teilnehmer:innen der Lehrveranstaltung. Das Ergebnis können Sie unter folgenden Link einsehen: . Formelverzeichnis Computational Intelligence in Games .",
            "url": "https://adockhorn.github.io/Boosting/boosting/gleichungen/herleitungen/2021/03/13/boosting-formelverzeichnis.html",
            "relUrl": "/boosting/gleichungen/herleitungen/2021/03/13/boosting-formelverzeichnis.html",
            "date": " • Mar 13, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Interaktive Lernmaterialien zum Thema Boosting",
            "content": "Boosting . Dieses Jupyter Notebook leitet durch die Entwicklung eines Adaboost Klassifikators und begleitet meinen Vortrag zur Lehrprobe and der Universität Rostock. In diesem Notebook werden die folgenden Themen besprochen: . Zunächst folgt der Import genereller Bibliotheken, welche im Verlauf dieses Notebooks verwendet werden: . import numpy as np # Matrizen und Vektoren from typing import Optional # type hints import matplotlib.pyplot as plt # Visualisierung der Ergebnisse import matplotlib as mpl # shortcut für einige Visualisierungsklassen # Implementierung von Entscheidungsbäumen from sklearn.tree import DecisionTreeClassifier # Funktionen zum Generieren von Trainingsbeispielen from sklearn.datasets import make_moons, make_circles, make_classification . Funktion zur Visualisierung der Ergebnisse . Um die Ergebnisse gelernter Klassifikatoren zu visualisieren, wird die folgende Funktion bereitgestellt. Hierbei wird davon ausgegangen, dass der Datensatz aus 2 dimensionalen Punkten besteht, welche jeweils durch +1 oder -1 gelabelet wurden. . Positiv zu klassifizierende Punkte werden durch ein rotes Plus-Zeichen dargestellt, wohingehen negativ zu klassifizierende Punkte durch einen blauen Kreis dargestellt werden. Optional kann die Ausgabe eines Klassifikators durch die rote oder blaue Hinterlegung des Hintergrunds dargestellt werden, wohingegen vom Klassifikator falsch klassifizierte Datenpunkte eingekreist werden. . Weitere optionale Funktionen werden in der Funktionsbeschreibung gelistet. . def plot_results (X: np.ndarray, y: np.ndarray, clf=None, sample_weights: Optional[np.ndarray] = None, annotate: bool = False, ax: Optional[mpl.axes.Axes] = None, size_factor=1, fontsize=16, pad = 1, highlight_errors=True) -&gt; None: &quot;&quot;&quot; Plot ± Trainingsdaten in 2D, optional kann die Ausgabe des Klassifikators als Hintergrundfarbe hinzugefügt werden, ebenfalls optional kann die Gewichtung als Label eines jeden Datenpunkts hinzugefügt werden :param X: Trainingsdaten :param y: Trainingslabels :param clf: Klassifikator :param sample_weights: Gewichte der Trainingsbeispiele :param annotate: if true: Schreibe die Werte aus sample_weights an jeden Datenpunkt :param ax: MPL-Axis object welches zum plotten des Ergebnis verwendet werden soll :param size_factor: Skalierung der Trainingsdatenpunkte, um Größenunterschiede hervorzuheben :param fontsize: fontsize sämtlicher Beschriftungen :param pad: Größe des weißen Bereichs links, rechts, über und unter den Datenpunkten :param highlight_errors: falsche Klassifikationen einkreisen :return: MPL axis object &quot;&quot;&quot; assert set(y) == {-1, 1}, &#39;Label müssen +1 oder -1 sein.&#39; # sollte keine Axis Object gegeben sein, wird eine neue Figure erstellt if not ax: fig, ax = plt.subplots(figsize=(5, 5), dpi=100) fig.set_facecolor(&#39;white&#39;) # Plotgrenzen abhängig von min und max Werten des Datensatzes und einem zusätzlichen weißen Rand x_min, x_max = X[:, 0].min() - pad, X[:, 0].max() + pad y_min, y_max = X[:, 1].min() - pad, X[:, 1].max() + pad # Berechnung der Markergrößen if sample_weights is not None: sizes = np.array(sample_weights) * X.shape[0] * 100 else: sizes = np.ones(shape=X.shape[0]) * 100 # plotten positiv zu klassifizierender Datenpunkte X_pos = X[y == 1] sizes_pos = sizes[y == 1] ax.scatter(*X_pos.T, s=sizes_pos*size_factor, marker=&#39;+&#39;, color=&#39;red&#39;) # plotten negativ zu klassifizierender Datenpunkte X_neg = X[y == -1] sizes_neg = sizes[y == -1] ax.scatter(*X_neg.T, s=sizes_neg*size_factor, marker=&#39;.&#39;, c=&#39;blue&#39;) if clf: # Ausgabe des Klassifikators als blau/rote Hinterlegung plot_step = 0.01 xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step), np.arange(y_min, y_max, plot_step)) Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) # If all predictions are positive class, adjust color map acordingly if list(np.unique(Z)) == [1]: fill_colors = [&#39;r&#39;] else: fill_colors = [&#39;b&#39;, &#39;r&#39;] ax.contourf(xx, yy, Z, colors=fill_colors, alpha=0.2) # einkreisen falsch klassifizierter Datenpunkte if highlight_errors: error = (clf.predict(X) != y) ax.scatter(*X[error].T, s=250, facecolors=&#39;none&#39;, edgecolors=&#39;black&#39;) # Beschriftung der Datenpunkte if annotate: if sample_weights is None: for i, (x, y) in enumerate(X): offset = 0.05 ax.annotate(f&#39;$x_{i + 1}$&#39;, (x + offset, y - offset)) else: for (i, (x, y)), weight in zip(enumerate(X), sample_weights): offset = 0.05 ax.annotate(f&#39;${weight}$&#39;, (x + offset, y + 0.1), fontsize=fontsize)# - offset)) # Achsenbeschriftung ax.set_xlim(x_min+0.5, x_max-0.5) ax.set_ylim(y_min+0.5, y_max-0.5) ax.set_xlabel(&#39;$x_1$&#39;, fontsize=16) ax.set_ylabel(&#39;$x_2$&#39;, fontsize=16) . Erstellen eines Trainingsdatensatzes . Wähle eine der folgenden Beispieldatensätze aus oder erzeuge einen eigenen. Für die korrekte Durchführung von Adaboost müssen die Label -1 und +1 für die Unterscheidung der beiden Klassen verwendet werden. . Die Funktion label_preprocessing kann verwendet werden um die Label von anderen Datensätzen zu transformieren. . def generate_trainingdata(): X = np.array([[1, 2], [1, 3], [2, 1], [2, 3], [3, 1], [3, 2], [3, 3], [2, 2]]) y = np.array([-1, -1, -1, +1, +1, -1, +1, -1]) return X, y def label_preprocessing(y): values = np.unique(y) # Labels sind bereits +1 oder -1 if 1 in values and -1 in values: return y assert len(values) == 2, &#39;Es müssen 2-Klassenprobleme verwendet werden!&#39; new_labels = np.zeros(y.shape) new_labels[y==values[0]] = -1 new_labels[y==values[1]] = +1 return new_labels # wähle einen der folgenden Trainingsdatensätze durch auskommentieren aller anderen X, y = generate_trainingdata() #X, y = make_classification(n_features=2, n_redundant=0, n_informative=2, random_state=1, n_clusters_per_class=1) #X, y = make_moons(noise=0.1) #X, y = make_circles(noise=0.1) # es kann nötig sein die Label auf +1 und -1 anzupassen. Die folgende Funktion übernimmt diese Aufgabe wenn genau 2 # Ziellabel existieren y = label_preprocessing(y) # Visualisierung der Trainingsdaten plot_results(X, y) . Lernen eines Entscheidungsbaums . Wir hatten bereits in einer vorherigen Veranstaltung die Implementierung von Entscheidungsbäumen thematisiert. Dieses mal greifen wir auf die Implementierung von scikit-learn zurück. . Eine Übersicht aller Parameter finden sie hier: . https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier . dt = DecisionTreeClassifier() # Durchführen des Trainings bezüglich der gegebenen Daten X und der Ziellabel y dt.fit(X, y) # Darstellung des Ergebnisses plot_results(X, y, dt) . Sklearn erlaubt die Begrenzung des Entscheidungsbaums anhang von Schwellenwerten. Um einen Entscheidungsstumpf zu lernen reduzieren wir die maximale Tiefe auf 1. Beachte die Fehlklassifikationen des Entscheidungsstump im nachfolgenden Plot. . stump = DecisionTreeClassifier(max_depth=1) stump.fit(X, y) plot_results(X, y, stump) . Definition von AdaBoost . In den folgenden Codeabschnitten werden wir schrittweise den Adaboost Classifier erstellen. Wir beginnen mit der Definition der Basisklasse, welche die Entscheidungsstümpfe, deren Gewichte, die Gewichte unserer Trainingsdaten und den Fehler des Klassifikators speichert. . class AdaBoost: &quot;&quot;&quot; AdaBoost enemble classifier from scratch &quot;&quot;&quot; def __init__(self, random_state=0): self.stumps = None # Placeholder für ein Array von Entscheidungsstümpfen self.stump_weights = None # Placeholder für ein Array von Klassifikatorgewichten self.errors = None # Placeholder für ein Array des gewichteten Trainingsfehlers self.sample_weights = None # Placeholder für ein Array der Trainingsbeispielgewichte self.random_state = random_state # random_state um bei gleichen seed die gleiche Ausgabe zu ermöglichen . Als nächstes fügen wir unserer AdaBoost-Klasse die Trainingsfunktionen hinzu. Hierbei überprüft die _check_X_y funktion ob die Ziellabel in der Menge -1 und +1 entsprechen. Die fit Funktion definiert den Trainingsablauf wie wir ihn kennengelernt haben. . def _check_X_y(self, X, y): &quot;&quot;&quot; Validate assumptions about format of input data :param self: benötigt, um als Memberfunktion aufrufbar zu sein :param X: Trainingsdaten beliebiger Dimensionszahl :param y: Ziellabel y_i in {-1, +1} :return: X, y &quot;&quot;&quot; assert set(y) == {-1, 1}, &#39;Response variable must be ±1&#39; return X, y def fit(self, X: np.ndarray, y: np.ndarray, iters: int): &quot;&quot;&quot; Fit the model using training data :param self: benötigt, um als Memberfunktion aufrufbar zu sein :param X: Trainingsdaten beliebiger Dimensionszahl :param y: Ziellabel y_i in {-1, +1} :param iters: Anzahl der zu lernenden Entscheidungsstümpfe :return: Adaboost Ensemble &quot;&quot;&quot; # Überprüfe Label X, y = self._check_X_y(X, y) # Initialisiere Arrays zum Speichern der Zwischenschritte n = X.shape[0] self.sample_weights = np.zeros(shape=(iters, n)) self.stumps = np.zeros(shape=iters, dtype=object) self.stump_weights = np.zeros(shape=iters) self.errors = np.zeros(shape=iters) # initialize weights uniformly self.sample_weights[0] = np.ones(shape=n) / n # lerne schrittweise einen neuen Klassifikator und füge ihn dem Array hinzu for t in range(iters): # definiere und trainiere Entscheidungsstumpf wie zuvor gesehen curr_sample_weights = self.sample_weights[t] stump = DecisionTreeClassifier(max_depth=1, max_leaf_nodes=2, random_state=self.random_state) stump = stump.fit(X, y, sample_weight=curr_sample_weights) # Berechne den gewichteten Fehler des neuen Klassifikators stump_pred = stump.predict(X) err = curr_sample_weights[(stump_pred != y)].sum() # Berechne das Gewicht des Klassifikators stump_weight = np.log((1 - err) / err) # Aktualisiere die Gewichte der Trainingsbeispiele new_sample_weights = ( curr_sample_weights * np.exp(-0.5*stump_weight * y * stump_pred) ) # Normalisiere die Gewichte der Trainingsbeispiele new_sample_weights /= new_sample_weights.sum() # Speicher die Gewichte für die nächste Iteration if t+1 &lt; iters: self.sample_weights[t+1] = new_sample_weights # Speichere den trainierten Entscheidungsstump self.stumps[t] = stump self.stump_weights[t] = stump_weight self.errors[t] = err return self # hinzufügen der soeben erstellen Methoden zur Klasse AdaBoost # Achtung: normalerweise nicht empfohlen, aber es ermöglicht uns an dieser Stelle die Schritte einzeln zu kommentieren AdaBoost._check_X_y = _check_X_y AdaBoost.fit = fit . Als nächstes fügen wir die Klassifikationsfunktionen hinzu. Predict_proba gibt die graduelle Ausgabe des Ensembles aus, wohingegen predict die Labels -1 und 1 ausgibt. . def predict(self, X): &quot;&quot;&quot; Gibt Label der Form -1 oder +1 aus :param self: benötigt, um als Memberfunktion aufrufbar zu sein :param X: Testdaten mit gleicher Dimensionszahl wie die Trainingsdaten :return: &quot;&quot;&quot; # bestimme das Vorzeichen der gewichteten Summe zur Ausgabe von +1 und -1 pro Datenpunkt return np.sign(self.predict_proba(X)) def predict_proba(self, X): &quot;&quot;&quot; Gibt die graduelle Ausgabe des Ensembles aus :param self: benötigt, um als Memberfunktion aufrufbar zu sein :param X: Testdaten mit gleicher Dimensionszahl wie die Trainingsdaten :return: &quot;&quot;&quot; # bestimme die Vorhersage jedes Klassifikators für jeden Datenpunkt X stump_preds = np.array([stump.predict(X) for stump in self.stumps]) # gib die gewichtete Summe aus return np.dot(self.stump_weights, stump_preds) # hinzufügen der soeben erstellen Methoden zur Klasse AdaBoost # Achtung: normalerweise nicht empfohlen, aber es ermöglicht uns an dieser Stelle die Schritte einzeln zu kommentieren AdaBoost.predict = predict AdaBoost.predict_proba = predict_proba . Es wird Zeit den Adaboost Klassifikator zu testen: . iters = 10 ada = AdaBoost() ada.fit(X, y, iters) plot_results(X, y, ada) . Im nächsten Schritt schauen wir uns die graduelle Ausgabe des Klassifikators an. . plot_results(X, y, ada, annotate=True, sample_weights=np.abs(np.round(ada.predict_proba(X),2)), size_factor=0.3, fontsize=12) . Um den Trainingsverlauf zu visualisieren, definieren wir eine Hilfsfunktion, welche ein bestehendes Ensemble auf die ersten t Klassifikatoren reduziert. . def truncate_adaboost(self, t: int): &quot;&quot;&quot; Truncate a fitted AdaBoost up to (and including) a particular iteration &quot;&quot;&quot; assert t &gt; 0, &#39;t must be a positive integer&#39; from copy import deepcopy new_clf = deepcopy(self) new_clf.stumps = self.stumps[:t] new_clf.stump_weights = self.stump_weights[:t] return new_clf AdaBoost.truncate_adaboost = truncate_adaboost . In der folgenden Darstellung zeigen die Plots in der ersten Reihe die gelernten Entscheidungsstümpfe, wohingegen die zweite Reihe den jeweils aktuellen Zustand des Ensembles darstellt. Von links nach rechts werden die Zeitschritte 1 bis iter dargestellt. . fig, ax = plt.subplots(2, iters, figsize=(iters*4,8)) for i in range(1, iters+1): truncated_ada = ada.truncate_adaboost(i) plot_results(X, y, truncated_ada.stumps[i-1], ax = ax[0, i-1]) plot_results(X, y, truncated_ada, ax = ax[1, i-1]) plt.tight_layout() . Analyse des Trainingverlaufs . Wir schauen uns nun die einzelnen Fehlerfunktionen im Vergleich an. Hierbei beobachten wir den Klassifikationsfehler, den gewichteten Klassifikationsfehler und den exponentiellen Fehler pro Iteration. . binary_error = np.zeros(iters) weighted_error = np.zeros(iters) exponential_error = np.zeros(iters) for i in range(1, iters+1): truncated_ada = ada.truncate_adaboost(i) binary_error[i-1] = np.sum(truncated_ada.predict(X) != y) weighted_error[i-1] = np.sum(np.array(truncated_ada.predict(X) != y) * truncated_ada.sample_weights[-1]) exponential_error[i-1] = np.sum(truncated_ada.sample_weights[-1] * np.exp(-0.5 * y * truncated_ada.predict_proba(X))) plt.plot(binary_error, linestyle = &quot;solid&quot;, label=&quot;binary error&quot;) plt.plot(weighted_error, linestyle = &quot;dashed&quot;, label=&quot;weighted error&quot;) plt.plot(exponential_error, linestyle = &quot;dotted&quot;, label=&quot;exponential error&quot;) plt.legend() plt.show() . Bei der vorherigen Darstellung ist zu sehen, dass, selbst wenn der Trainingsfehler bereits 0 ist, AdaBoost weiter den exponentiellen Fehler minimiert. Insbesondere bei komplexen Datensätzen kommt es hierdurch zur Maximierung des Abstands zwischen Trainingspunkten und der Trennebene des Klassifikators. . Eine solche Maximierung des Margins ist uns bereits aus Support Vector Maschinen bekannt! . Overfitting und Early Stopping . Durch den permanten Drang den exponentiellen Fehler zu minimieren, wird Adaboost im Falle von Outliern irgendwann overfitten. Dies kann jedoch eine enorme Anzahl an Iterationen dauern. . Da die recheneffiziente Matrixmultiplikation einen zu hohen RAM-Bedarf hat, tauschen wir die predict_proba Funktion mit einer weniger schnellen, dafür auch weniger speicheraufwändigen Version aus. . def predict_proba(self, X): &quot;&quot;&quot; Gibt die graduelle Ausgabe des Ensembles aus :param self: benötigt, um als Memberfunktion aufrufbar zu sein :param X: Testdaten mit gleicher Dimensionszahl wie die Trainingsdaten :return: &quot;&quot;&quot; # bestimme die Vorhersage jedes Klassifikators für jeden Datenpunkt X result = np.zeros(X.shape[0]) for stump, weight in zip(self.stumps, self.stump_weights): result += stump.predict(X) * weight # gib die gewichtete Summe aus return result AdaBoost.predict_proba = predict_proba X, y = make_moons(300, noise=0.4, random_state=0) y = label_preprocessing(y) plot_results(X, y) ada = AdaBoost() ada.fit(X,y, 5000) . &lt;__main__.AdaBoost at 0x1d088ce8&gt; . Das folgende Beispiel stellt das Ensemble nach 1, 5, 10, 50, 100, 500, 1000, und 5000 Schritten dar. Beobachte wie das Ensemble bereits nach 50 Schritten eine grobe Unterteilung der beiden Klassen ermöglicht. Insbesondere in den letzten beiden dargestellten Ensemblen (t=1000 und t=5000) zerfallen die sauberen Grenzen des Ensembles und immer mehr Ausreißer werden durch das Ensemble abgebildet. . Durch die Verknüpfung einzelner stark gewichteter Entscheidungsstümpfe entsteht ein Flickenteppich aus positiv und negativ zu klassifizierenden Bereichen. . fig, ax = plt.subplots(4, 2, figsize=(8, 16)) for i, t in enumerate([1, 5, 10, 50, 100, 500, 1000, 5000]): truncated_ada = ada.truncate_adaboost(t) plot_results(X, y, truncated_ada, ax=ax[i//2, i%2], size_factor=0.1, highlight_errors=False) ax[i//2, i%2].set_title(&quot;Iteration &quot; + str(t)) plt.tight_layout() plt.savefig(&quot;ensemble.pdf&quot;) plt.show() . Diese Überanpassung lässt sich wie bereits erwähnt durch die dauerhafte Anpassung an den exponentiellen Fehler erklären. Diese Anpassung wird deutlich, wenn man den binären Fehler und den exponentiellen Fehler über den Verlauf der Iterationen plottet. . binary_error = np.zeros(5000) exponential_error = np.zeros(5000) prediction = np.zeros(X.shape[0]) for i in range(1, 5001): np.zeros(X.shape[0]) prediction += ada.stumps[i-1].predict(X) * ada.stump_weights[i-1] labels = np.sign(prediction) binary_error[i-1] = np.sum(labels != y) / X.shape[0] exponential_error[i-1] = np.sum(ada.sample_weights[i-1] * np.exp(-0.5 * y * prediction)) plt.plot(binary_error, linestyle = &quot;solid&quot;, label=r&quot;binary error / $ vert D vert$&quot;) plt.plot(exponential_error, linestyle = &quot;dotted&quot;, label=&quot;exponential error&quot;) plt.legend() plt.show() . Vermeidung von Overfitting . Wie auch bei anderen Klassifikatoren, können wir dieses Problem vermeiden, wenn wir den Datensatz zunächst in einen Trainings- und Testdatensatz aufteilen und den Fehler über den Verlauf der Iterationen auf diesen beiden Datensätzen beobachten. . Hierfür teilen wir zunächst den Datensatz in zwei Mengen auf wobei 70% der Beispiele als Trainingsdaten und 30% als Testdaten verwendet werden. Nachfolgend trainieren wir AdaBoost auf den Trainingsdaten. . from sklearn.model_selection import train_test_split [X_train, X_test, y_train, y_test] = train_test_split(X, y, test_size=0.3) ada = AdaBoost() ada.fit(X_train,y_train, 1000) . &lt;__main__.AdaBoost at 0x1e925898&gt; . Als nächstes messen wir den binären Fehler der Trainings- und Testdaten per Iteration und stellen diese in gewohnter Weise grafisch dar. . binary_error_train = np.zeros(1000) binary_error_test = np.zeros(1000) prediction_train = np.zeros(X_train.shape[0]) prediction_test = np.zeros(X_test.shape[0]) for i in range(1, 1001): prediction_train += ada.stumps[i-1].predict(X_train) * ada.stump_weights[i-1] prediction_test += ada.stumps[i-1].predict(X_test) * ada.stump_weights[i-1] binary_error_train[i-1] = np.sum(np.sign(prediction_train) != y_train) / X_train.shape[0] binary_error_test[i-1] = np.sum(np.sign(prediction_test) != y_test) / X_test.shape[0] plt.plot(binary_error_train, linestyle = &quot;solid&quot;, label=r&quot;binary error train&quot;) plt.plot(binary_error_test, linestyle = &quot;dotted&quot;, label=&quot;binary error test&quot;) plt.legend() plt.show() . Nach einer anfänglichen Phase in welcher der Klassifikationsfehler für die Trainings- und Testdaten reduziert werden konnte, stagniert die Verbesserung bezüglich der Testdaten und steigt phasenweise sogar wieder an. Demnach findet von diesem Moment an nur noch eine Überanpassung an den Trainingsdatensatz statt. In diesem Fall können wir das Training vorzeitig abbrechen. . Wir reduzieren den Plot auf die ersten 100 Zeitschritte um den Abbruchpunkt bestmöglich zu bestimmen. . binary_error_train = np.zeros(100) binary_error_test = np.zeros(100) prediction_train = np.zeros(X_train.shape[0]) prediction_test = np.zeros(X_test.shape[0]) for i in range(1, 101): prediction_train += ada.stumps[i-1].predict(X_train) * ada.stump_weights[i-1] prediction_test += ada.stumps[i-1].predict(X_test) * ada.stump_weights[i-1] binary_error_train[i-1] = np.sum(np.sign(prediction_train) != y_train) / X_train.shape[0] binary_error_test[i-1] = np.sum(np.sign(prediction_test) != y_test) / X_test.shape[0] plt.plot(binary_error_train, linestyle = &quot;solid&quot;, label=r&quot;binary error train&quot;) plt.plot(binary_error_test, linestyle = &quot;dotted&quot;, label=&quot;binary error test&quot;) plt.legend() plt.show() . Die genaue Bestimmung des Abbruchzeitpunkts ist zumeist schwer zu bestimmen. Ziel ist es, den Trainingsfehler und den Testfehler zu minimieren. Zumeist steigt der Testfehler aber wieder an, während der Trainingsfehler weiter sinkt. Im folgenden Beispiel werden die Zeitunkte 20, 30, 40 und 450 miteinander verglichen. . Durch das Hinzufügen weiterer Klassifikatoren in das Ensemble, können zunehmend mehr Trainingsdaten richtig klassifiziert werden. Hierfür steigt die Komplexität der Entscheidungsgrenze des Ensembles und nimmt langsam die Form der Datenverteilung an. Gegeben der folgenden 4 Bilder scheint 40 ein interessanter Abbruchpunkt zu sein. . fig, ax = plt.subplots(1, 4, figsize=(16, 4)) for i, t in enumerate([20, 30, 40, 450]): truncated_ada = ada.truncate_adaboost(t) plot_results(X_train, y_train, truncated_ada, ax=ax[i], size_factor=0.1, highlight_errors=False) ax[i].set_title(&quot;Iteration &quot; + str(t)) plt.tight_layout() plt.show() .",
            "url": "https://adockhorn.github.io/Boosting/fastpages/jupyter/boosting/implementation/2020/02/21/introducing-fastpages.html",
            "relUrl": "/fastpages/jupyter/boosting/implementation/2020/02/21/introducing-fastpages.html",
            "date": " • Feb 21, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://adockhorn.github.io/Boosting/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://adockhorn.github.io/Boosting/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}
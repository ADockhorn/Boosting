{
  
    
        "post0": {
            "title": "Weiterführende Quellen",
            "content": "Im Vortrag benannte Quellen . Surowiecki, J. (2004). The wisdom of crowds. Why the many are smarter than the few and how collective wisdom shapes business, economies, societies and nations. Abacus das vorgestellte Beispiel und zahlreiche weitere Vergleiche von Gruppen- und Expertenentscheidungen | . | Kearns, M. and Valiant, L.G. (1989). Cryptographic limitations on learning Boolean formulae and finite automata. Proceedings of the Twenty-First Annual ACM Symposium on Theory of Computing (pp. 433-444). New York, NY: ACM Press. &lt;/li&gt; https://www.cis.upenn.edu/~mkearns/papers/cryptojacm.pdf”&gt;https://www.cis.upenn.edu/~mkearns/papers/cryptojacm.pdf | Sind Weak und Strong Learnability unterschiedliche Klassen? | . | Schapire, R. E. (1990). The strength of weak learnability. Machine learning, 5(2), 197-227. https://link.springer.com/content/pdf/10.1007/BF00116037.pdf | erste Nachweis von effektiven Ensemblen aus “Weak learnern” | . | Freund, Y., &amp; Schapire, R. E. (1997). A decision-theoretic generalization of on-line learning and an application to boosting. Journal of computer and system sciences, 55(1), 119-139. https://www.face-rec.org/algorithms/Boosting-Ensemble/decision-theoretic_generalization.pdf | Vorstellung des AdaBoost Algorithmus, Gewichtung eines Weak Learners basierend auf seiner Korrektheit | . | Llew Mason, Jonathan Baxter, Peter Bartlett, and Marcus Frean (2000); Boosting Algorithms as Gradient Descent, in S. A. Solla, T. K. Leen, and K.-R. Muller, editors, Advances in Neural Information Processing Systems 12, pp. 512-518, MIT Press https://papers.nips.cc/paper/1999/file/96a93ba89a5b5c6c226e49b88973f46e-Paper.pdf | Boosting als Gradientenabstieg im Funktionsraum&lt;/li&gt; | . | . Ensemble Algorithmen . Schapire, R. E. &amp; Freund, Y. (2012). Boosting: foundations and algorithms. MIT Press https://mitpress.mit.edu/sites/default/files/titles/content/boosting_foundations_algorithms/chapter005.html | eine alternative Interpretation des AdaBoost Algorthmus und dessen Trainingsverhaltens | auch die anderen Kapitel sind sehr zu empfehlen | . | Zhang, C., &amp; Ma, Y. (Eds.). (2012). Ensemble Machine Learning. Springer US. https://doi.org/10.1007/978-1-4419-9326-7 gut verständliche Kapitel zu den Themen: Ensemble Algorithmen (Kapitel 1) | Boosting Algorithmen (Kapitel 2) | Random Forests (Kapitel 3) | . | . | . Generelle Bücher zum Thema “Intelligent Data Analytics” . Die folgenden beiden Bücher repräsentieren gut verständliche Einführungen in Methoden der künstlichen Intelligenz und der intelligenten Datenanalyse. Da keine Web-Version verfügbar ist, verweise ich auf die örtliche Universitätsbibliothek, welche beide Bücher vorrätig hat. . Russell, J. S., &amp; Norvig, P. (2003). Artificial Intelligence: A Modern Approach. In Artificial Intelligence. https://doi.org/10.1017/S0269888900007724 https://find.ub.uni-rostock.de/id%7Bcolon%7D359882528 | . | Kruse, R., Borgelt, C., Braune, C., Mostaghim, S., &amp; Steinbrecher, M. (2016). Computational Intelligence. In Computational Intelligence (2nd Editio). Springer London. https://doi.org/10.1007/978-1-4471-7296-3 https://find.ub.uni-rostock.de/id%7Bcolon%7D843911816 | . | .",
            "url": "https://adockhorn.github.io/Boosting/boosting/ensembles/quellen/2021/03/13/weiterfuehrende-quellen.html",
            "relUrl": "/boosting/ensembles/quellen/2021/03/13/weiterfuehrende-quellen.html",
            "date": " • Mar 13, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Formelverzeichnis Boosting",
            "content": "Formelverzeichnis . Nachfolgend finden Sie ein ausführliches Formelverzeichnis und eine Übersicht der besprochenen Herleitungen: . Formelverzeichnis Boosting . Ein vergleichbares Formelverzeichnis habe ich bereits während meiner Lehre an der Otto-von-Guericke Universität in Magdeburg erstellt. Begleitend zu meiner Lehrvertretung der Vorlesung “Computational Intelligence in Games” entstand dieses in Zusammenarbeit mit Teilnehmer:innen der Lehrveranstaltung. Das Ergebnis können Sie unter folgenden Link einsehen: . Formelverzeichnis Computational Intelligence in Games .",
            "url": "https://adockhorn.github.io/Boosting/boosting/gleichungen/herleitungen/2021/03/13/boosting-formelverzeichnis.html",
            "relUrl": "/boosting/gleichungen/herleitungen/2021/03/13/boosting-formelverzeichnis.html",
            "date": " • Mar 13, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Interaktive Lernmaterialien zum Thema Boosting",
            "content": "Boosting . Dieses Jupyter Notebook leitet durch die Entwicklung eines Adaboost Klassifikators und begleitet meinen Vortrag. Sie können eine interaktive Version dieses Notebooks starten, indem sie auf die Buttons launch Binder oder Open in Colab klicken. . Zunächst folgt der Import genereller Bibliotheken, welche im Verlauf dieses Notebooks verwendet werden: . import numpy as np # Matrizen und Vektoren from typing import Optional # type hints import matplotlib.pyplot as plt # Visualisierung der Ergebnisse import matplotlib as mpl # shortcut für einige Visualisierungsklassen # Implementierung von Entscheidungsbäumen from sklearn.tree import DecisionTreeClassifier # Funktionen zum Generieren von Trainingsbeispielen from sklearn.datasets import make_moons, make_circles, make_classification . Funktion zur Visualisierung der Ergebnisse . Um die Ergebnisse gelernter Klassifikatoren zu visualisieren, wird die folgende Funktion bereitgestellt. Hierbei wird davon ausgegangen, dass der Datensatz aus 2-dimensionalen Punkten besteht, welchen die Ziellabel +1 oder -1 zugewiesen werden sollen. . Positiv zu klassifizierende Punkte werden durch ein rotes Plus-Zeichen dargestellt, wohingehen negativ zu klassifizierende Punkte durch einen blauen Kreis dargestellt werden. Optional kann die Ausgabe eines Klassifikators durch die rote oder blaue Hinterlegung des Hintergrunds dargestellt werden, wohingegen vom Klassifikator falsch klassifizierte Datenpunkte eingekreist werden. . Weitere optionale Funktionen werden in der Funktionsbeschreibung gelistet. . def plot_results (X: np.ndarray, y: np.ndarray, clf=None, sample_weights: Optional[np.ndarray] = None, annotate: bool = False, ax: Optional[mpl.axes.Axes] = None, size_factor=1, fontsize=16, pad = 1, highlight_errors=True) -&gt; None: &quot;&quot;&quot; Plot ± Trainingsdaten in 2D, optional kann die Ausgabe des Klassifikators als Hintergrundfarbe hinzugefügt werden, ebenfalls optional kann die Gewichtung als Label eines jeden Datenpunkts hinzugefügt werden :param X: Trainingsdaten :param y: Trainingslabels :param clf: Klassifikator :param sample_weights: Gewichte der Trainingsbeispiele :param annotate: if true: Schreibe die Werte aus sample_weights an jeden Datenpunkt :param ax: MPL-Axis object welches zum plotten des Ergebnis verwendet werden soll :param size_factor: Skalierung der Trainingsdatenpunkte, um Größenunterschiede hervorzuheben :param fontsize: fontsize sämtlicher Beschriftungen :param pad: Größe des weißen Bereichs links, rechts, über und unter den Datenpunkten :param highlight_errors: falsche Klassifikationen einkreisen :return: MPL axis object &quot;&quot;&quot; assert set(y) == {-1, 1}, &#39;Label müssen +1 oder -1 sein.&#39; # sollte keine Axis Object gegeben sein, wird eine neue Figure erstellt if not ax: fig, ax = plt.subplots(figsize=(5, 5), dpi=100) fig.set_facecolor(&#39;white&#39;) # Plotgrenzen abhängig von min und max Werten des Datensatzes und einem zusätzlichen weißen Rand x_min, x_max = X[:, 0].min() - pad, X[:, 0].max() + pad y_min, y_max = X[:, 1].min() - pad, X[:, 1].max() + pad # Berechnung der Markergrößen if sample_weights is not None: sizes = np.array(sample_weights) * X.shape[0] * 100 else: sizes = np.ones(shape=X.shape[0]) * 100 # plotten positiv zu klassifizierender Datenpunkte X_pos = X[y == 1] sizes_pos = sizes[y == 1] ax.scatter(*X_pos.T, s=sizes_pos*size_factor, marker=&#39;+&#39;, color=&#39;red&#39;) # plotten negativ zu klassifizierender Datenpunkte X_neg = X[y == -1] sizes_neg = sizes[y == -1] ax.scatter(*X_neg.T, s=sizes_neg*size_factor, marker=&#39;.&#39;, c=&#39;blue&#39;) if clf: # Ausgabe des Klassifikators als blau/rote Hinterlegung plot_step = 0.01 xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step), np.arange(y_min, y_max, plot_step)) Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) # If all predictions are positive class, adjust color map acordingly if list(np.unique(Z)) == [1]: fill_colors = [&#39;r&#39;] else: fill_colors = [&#39;b&#39;, &#39;r&#39;] ax.contourf(xx, yy, Z, colors=fill_colors, alpha=0.2) # einkreisen falsch klassifizierter Datenpunkte if highlight_errors: error = (clf.predict(X) != y) ax.scatter(*X[error].T, s=250, facecolors=&#39;none&#39;, edgecolors=&#39;black&#39;) # Beschriftung der Datenpunkte if annotate: if sample_weights is None: for i, (x, y) in enumerate(X): offset = 0.05 ax.annotate(f&#39;$x_{i + 1}$&#39;, (x + offset, y - offset)) else: for (i, (x, y)), weight in zip(enumerate(X), sample_weights): offset = 0.05 ax.annotate(f&#39;${weight}$&#39;, (x + offset, y + 0.1), fontsize=fontsize)# - offset)) # Achsenbeschriftung ax.set_xlim(x_min+0.5, x_max-0.5) ax.set_ylim(y_min+0.5, y_max-0.5) ax.set_xlabel(&#39;$x_1$&#39;, fontsize=16) ax.set_ylabel(&#39;$x_2$&#39;, fontsize=16) . Erstellen eines Trainingsdatensatzes . Wähle eine der folgenden Beispieldatensätze aus oder erzeuge einen eigenen. Für die korrekte Durchführung von Adaboost müssen die Label -1 und +1 für die Unterscheidung der beiden Klassen verwendet werden. . Die Funktion label_preprocessing kann verwendet werden um die Label von anderen Datensätzen zu transformieren. . def generate_trainingdata(): X = np.array([[1, 2], [1, 3], [2, 1], [2, 3], [3, 1], [3, 2], [3, 3], [2, 2]]) y = np.array([-1, -1, -1, +1, +1, -1, +1, -1]) return X, y def label_preprocessing(y): values = np.unique(y) # Labels sind bereits +1 oder -1 if 1 in values and -1 in values: return y assert len(values) == 2, &#39;Es müssen 2-Klassenprobleme verwendet werden!&#39; new_labels = np.zeros(y.shape) new_labels[y==values[0]] = -1 new_labels[y==values[1]] = +1 return new_labels # wähle einen der folgenden Trainingsdatensätze durch auskommentieren aller anderen X, y = generate_trainingdata() #X, y = make_classification(n_features=2, n_redundant=0, n_informative=2, random_state=1, n_clusters_per_class=1) #X, y = make_moons(noise=0.1) #X, y = make_circles(noise=0.1) # es kann nötig sein die Label auf +1 und -1 anzupassen. Die folgende Funktion übernimmt diese Aufgabe wenn genau 2 # Ziellabel existieren y = label_preprocessing(y) # Visualisierung der Trainingsdaten plot_results(X, y) . Lernen eines Entscheidungsbaums . Wir hatten bereits in einer vorherigen Veranstaltung die Implementierung von Entscheidungsbäumen thematisiert. Dieses mal greifen wir auf die Implementierung von scikit-learn zurück. . Eine Übersicht aller Parameter ist auf der folgenden Website zu finden: . https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier . dt = DecisionTreeClassifier() # Durchführen des Trainings bezüglich der gegebenen Daten X und der Ziellabel y dt.fit(X, y) # Darstellung des Ergebnisses plot_results(X, y, dt) . Sklearn erlaubt die Begrenzung des Entscheidungsbaums anhand von Schwellenwerten. Um einen Entscheidungsstumpf zu lernen, reduzieren wir die maximale Tiefe auf 1. Beachte die Fehlklassifikationen des Entscheidungsstumpfs im nachfolgenden Plot. . stump = DecisionTreeClassifier(max_depth=1) stump.fit(X, y) plot_results(X, y, stump) . Definition von AdaBoost . In den folgenden Codeabschnitten werden wir schrittweise den Adaboost Classifier erstellen. Wir beginnen mit der Definition der Basisklasse, welche die Entscheidungsstümpfe, deren Gewichte, die Gewichte unserer Trainingsdaten und den Fehler des Klassifikators speichert. . class AdaBoost: &quot;&quot;&quot; AdaBoost enemble classifier from scratch &quot;&quot;&quot; def __init__(self, random_state=0): self.stumps = None # Placeholder für ein Array von Entscheidungsstümpfen self.stump_weights = None # Placeholder für ein Array von Klassifikatorgewichten self.errors = None # Placeholder für ein Array des gewichteten Trainingsfehlers self.sample_weights = None # Placeholder für ein Array der Trainingsbeispielgewichte self.random_state = random_state # random_state um bei gleichen seed die gleiche Ausgabe zu ermöglichen . Als Nächstes fügen wir unserer AdaBoost-Klasse die Trainingsfunktionen hinzu. Hierbei überprüft die _check_X_y funktion ob die Ziellabel in der Menge -1 und +1 entsprechen. Die fit Funktion definiert den Trainingsablauf wie wir ihn kennengelernt haben. . def _check_X_y(self, X, y): &quot;&quot;&quot; Validate assumptions about format of input data :param self: benötigt, um als Memberfunktion aufrufbar zu sein :param X: Trainingsdaten beliebiger Dimensionszahl :param y: Ziellabel y_i in {-1, +1} :return: X, y &quot;&quot;&quot; assert set(y) == {-1, 1}, &#39;Response variable must be ±1&#39; return X, y def fit(self, X: np.ndarray, y: np.ndarray, iters: int): &quot;&quot;&quot; Fit the model using training data :param self: benötigt, um als Memberfunktion aufrufbar zu sein :param X: Trainingsdaten beliebiger Dimensionszahl :param y: Ziellabel y_i in {-1, +1} :param iters: Anzahl der zu lernenden Entscheidungsstümpfe :return: Adaboost Ensemble &quot;&quot;&quot; # Überprüfe Label X, y = self._check_X_y(X, y) # Initialisiere Arrays zum Speichern der Zwischenschritte n = X.shape[0] self.sample_weights = np.zeros(shape=(iters, n)) self.stumps = np.zeros(shape=iters, dtype=object) self.stump_weights = np.zeros(shape=iters) self.errors = np.zeros(shape=iters) # initialize weights uniformly self.sample_weights[0] = np.ones(shape=n) / n # lerne schrittweise einen neuen Klassifikator und füge ihn dem Array hinzu for t in range(iters): # definiere und trainiere Entscheidungsstumpf wie zuvor gesehen curr_sample_weights = self.sample_weights[t] stump = DecisionTreeClassifier(max_depth=1, max_leaf_nodes=2, random_state=self.random_state) stump = stump.fit(X, y, sample_weight=curr_sample_weights) # Berechne den gewichteten Fehler des neuen Klassifikators stump_pred = stump.predict(X) err = curr_sample_weights[(stump_pred != y)].sum() # Berechne das Gewicht des Klassifikators stump_weight = np.log((1 - err) / err) # Aktualisiere die Gewichte der Trainingsbeispiele new_sample_weights = ( curr_sample_weights * np.exp(-0.5*stump_weight * y * stump_pred) ) # Normalisiere die Gewichte der Trainingsbeispiele new_sample_weights /= new_sample_weights.sum() # Speicher die Gewichte für die nächste Iteration if t+1 &lt; iters: self.sample_weights[t+1] = new_sample_weights # Speichere den trainierten Entscheidungsstump self.stumps[t] = stump self.stump_weights[t] = stump_weight self.errors[t] = err return self # hinzufügen der soeben erstellen Methoden zur Klasse AdaBoost # Achtung: normalerweise nicht empfohlen, aber es ermöglicht uns an dieser Stelle die Schritte einzeln zu kommentieren AdaBoost._check_X_y = _check_X_y AdaBoost.fit = fit . Als Nächstes fügen wir die Klassifikationsfunktionen hinzu. Predict_proba gibt die graduelle Ausgabe des Ensembles aus, wohingegen predict die Labels -1 und 1 ausgibt. . def predict(self, X): &quot;&quot;&quot; Gibt Label der Form -1 oder +1 aus :param self: benötigt, um als Memberfunktion aufrufbar zu sein :param X: Testdaten mit gleicher Dimensionszahl wie die Trainingsdaten :return: &quot;&quot;&quot; # bestimme das Vorzeichen der gewichteten Summe zur Ausgabe von +1 und -1 pro Datenpunkt return np.sign(self.predict_proba(X)) def predict_proba(self, X): &quot;&quot;&quot; Gibt die graduelle Ausgabe des Ensembles aus :param self: benötigt, um als Memberfunktion aufrufbar zu sein :param X: Testdaten mit gleicher Dimensionszahl wie die Trainingsdaten :return: &quot;&quot;&quot; # bestimme die Vorhersage jedes Klassifikators für jeden Datenpunkt X stump_preds = np.array([stump.predict(X) for stump in self.stumps]) # gib die gewichtete Summe aus return np.dot(self.stump_weights, stump_preds) # hinzufügen der soeben erstellen Methoden zur Klasse AdaBoost # Achtung: normalerweise nicht empfohlen, aber es ermöglicht uns an dieser Stelle die Schritte einzeln zu kommentieren AdaBoost.predict = predict AdaBoost.predict_proba = predict_proba . Es wird Zeit den Adaboost Klassifikator zu testen: . iters = 10 ada = AdaBoost() ada.fit(X, y, iters) plot_results(X, y, ada) . Im nächsten Schritt schauen wir uns die graduelle Ausgabe des Klassifikators an. . plot_results(X, y, ada, annotate=True, sample_weights=np.abs(np.round(ada.predict_proba(X),2)), size_factor=0.3, fontsize=12) . Um den Trainingsverlauf zu visualisieren, definieren wir eine Hilfsfunktion, welche ein bestehendes Ensemble auf die ersten t Klassifikatoren reduziert. . def truncate_adaboost(self, t: int): &quot;&quot;&quot; Truncate a fitted AdaBoost up to (and including) a particular iteration &quot;&quot;&quot; assert t &gt; 0, &#39;t must be a positive integer&#39; from copy import deepcopy new_clf = deepcopy(self) new_clf.stumps = self.stumps[:t] new_clf.stump_weights = self.stump_weights[:t] return new_clf AdaBoost.truncate_adaboost = truncate_adaboost . In der folgenden Darstellung zeigen die Plots in der ersten Reihe die gelernten Entscheidungsstümpfe, wohingegen die zweite Reihe den jeweils aktuellen Zustand des Ensembles darstellt. Von links nach rechts werden die Zeitschritte 1 bis iters dargestellt. . fig, ax = plt.subplots(2, iters, figsize=(iters*4,8)) for i in range(1, iters+1): truncated_ada = ada.truncate_adaboost(i) plot_results(X, y, truncated_ada.stumps[i-1], ax = ax[0, i-1]) plot_results(X, y, truncated_ada, ax = ax[1, i-1]) plt.tight_layout() . Analyse des Trainingverlaufs . Wir schauen uns nun die einzelnen Fehlerfunktionen im Vergleich an. Hierbei beobachten wir den Klassifikationsfehler, den gewichteten Klassifikationsfehler und den exponentiellen Fehler pro Iteration. . binary_error = np.zeros(iters) weighted_error = np.zeros(iters) exponential_error = np.zeros(iters) for i in range(1, iters+1): truncated_ada = ada.truncate_adaboost(i) binary_error[i-1] = np.sum(truncated_ada.predict(X) != y) weighted_error[i-1] = np.sum(np.array(truncated_ada.predict(X) != y) * truncated_ada.sample_weights[-1]) exponential_error[i-1] = np.sum(truncated_ada.sample_weights[-1] * np.exp(-0.5 * y * truncated_ada.predict_proba(X))) plt.plot(binary_error, linestyle = &quot;solid&quot;, label=&quot;binary error&quot;) plt.plot(weighted_error, linestyle = &quot;dashed&quot;, label=&quot;weighted error&quot;) plt.plot(exponential_error, linestyle = &quot;dotted&quot;, label=&quot;exponential error&quot;) plt.legend() plt.show() . Bei der vorherigen Darstellung ist zu sehen, dass, selbst wenn der Trainingsfehler bereits 0 ist, AdaBoost weiter den exponentiellen Fehler minimiert. Insbesondere bei komplexen Datensätzen kommt es hierdurch zur Maximierung des Abstands zwischen Trainingspunkten und der Trennebene des Klassifikators. . Eine solche Maximierung des Margins ist uns bereits aus Support Vector Maschinen bekannt! . Overfitting und Early Stopping . Durch den permanten Drang den exponentiellen Fehler zu minimieren, wird Adaboost im Falle von Outliern irgendwann overfitten. Dies kann jedoch eine enorme Anzahl an Iterationen dauern. . Da die recheneffiziente Matrixmultiplikation einen zu hohen RAM-Bedarf hat, tauschen wir die predict_proba Funktion mit einer weniger schnellen, dafür auch weniger speicheraufwändigen Version aus. . def predict_proba(self, X): &quot;&quot;&quot; Gibt die graduelle Ausgabe des Ensembles aus :param self: benötigt, um als Memberfunktion aufrufbar zu sein :param X: Testdaten mit gleicher Dimensionszahl wie die Trainingsdaten :return: &quot;&quot;&quot; # bestimme die Vorhersage jedes Klassifikators für jeden Datenpunkt X result = np.zeros(X.shape[0]) for stump, weight in zip(self.stumps, self.stump_weights): result += stump.predict(X) * weight # gib die gewichtete Summe aus return result AdaBoost.predict_proba = predict_proba X, y = make_moons(300, noise=0.4, random_state=0) y = label_preprocessing(y) plot_results(X, y) ada = AdaBoost() ada.fit(X,y, 5000) . &lt;__main__.AdaBoost at 0x1d088ce8&gt; . Das folgende Beispiel stellt das Ensemble nach 1, 5, 10, 50, 100, 500, 1000, und 5000 Schritten dar. Man beobachte, wie das Ensemble bereits nach 50 Schritten eine grobe Unterteilung der beiden Klassen ermöglicht. Insbesondere in den letzten beiden dargestellten Ensembles (t=1000 und t=5000) zerfallen die sauberen Grenzen des Ensembles und immer mehr Ausreißer werden durch das Ensemble abgebildet. . Durch die Verknüpfung einzelner stark gewichteter Entscheidungsstümpfe entsteht ein Flickenteppich aus positiv und negativ zu klassifizierenden Bereichen. . fig, ax = plt.subplots(4, 2, figsize=(8, 16)) for i, t in enumerate([1, 5, 10, 50, 100, 500, 1000, 5000]): truncated_ada = ada.truncate_adaboost(t) plot_results(X, y, truncated_ada, ax=ax[i//2, i%2], size_factor=0.1, highlight_errors=False) ax[i//2, i%2].set_title(&quot;Iteration &quot; + str(t)) plt.tight_layout() plt.savefig(&quot;ensemble.pdf&quot;) plt.show() . Diese Überanpassung lässt sich wie bereits erwähnt durch die dauerhafte Anpassung an den exponentiellen Fehler erklären. Diese Anpassung wird deutlich, wenn man den binären Fehler und den exponentiellen Fehler über den Verlauf der Iterationen plottet. . binary_error = np.zeros(5000) exponential_error = np.zeros(5000) prediction = np.zeros(X.shape[0]) for i in range(1, 5001): np.zeros(X.shape[0]) prediction += ada.stumps[i-1].predict(X) * ada.stump_weights[i-1] labels = np.sign(prediction) binary_error[i-1] = np.sum(labels != y) / X.shape[0] exponential_error[i-1] = np.sum(ada.sample_weights[i-1] * np.exp(-0.5 * y * prediction)) plt.plot(binary_error, linestyle = &quot;solid&quot;, label=r&quot;binary error / $ vert D vert$&quot;) plt.plot(exponential_error, linestyle = &quot;dotted&quot;, label=&quot;exponential error&quot;) plt.legend() plt.show() . Wie auch bei anderen Klassifikatoren können wir dieses Problem vermeiden, wenn wir den Datensatz zunächst in einen Trainings- und Testdatensatz aufteilen und den Fehler über den Verlauf der Iterationen auf diesen beiden Datensätzen beobachten. . Hierfür teilen wir zunächst den Datensatz in zwei Mengen auf, wobei 70% der Beispiele als Trainingsdaten und 30% als Testdaten verwendet werden. Nachfolgend trainieren wir AdaBoost auf den Trainingsdaten. . from sklearn.model_selection import train_test_split [X_train, X_test, y_train, y_test] = train_test_split(X, y, test_size=0.3) ada = AdaBoost() ada.fit(X_train,y_train, 1000) . &lt;__main__.AdaBoost at 0x1e925898&gt; . Als Nächstes messen wir den binären Fehler der Trainings- und Testdaten per Iteration und stellen diese in gewohnter Weise grafisch dar. . binary_error_train = np.zeros(1000) binary_error_test = np.zeros(1000) prediction_train = np.zeros(X_train.shape[0]) prediction_test = np.zeros(X_test.shape[0]) for i in range(1, 1001): prediction_train += ada.stumps[i-1].predict(X_train) * ada.stump_weights[i-1] prediction_test += ada.stumps[i-1].predict(X_test) * ada.stump_weights[i-1] binary_error_train[i-1] = np.sum(np.sign(prediction_train) != y_train) / X_train.shape[0] binary_error_test[i-1] = np.sum(np.sign(prediction_test) != y_test) / X_test.shape[0] plt.plot(binary_error_train, linestyle = &quot;solid&quot;, label=r&quot;binary error train&quot;) plt.plot(binary_error_test, linestyle = &quot;dotted&quot;, label=&quot;binary error test&quot;) plt.legend() plt.show() . Nach einer anfänglichen Phase in welcher der Klassifikationsfehler für die Trainings- und Testdaten reduziert werden konnte, stagniert die Verbesserung bezüglich der Testdaten und steigt phasenweise sogar wieder an. Demnach findet von diesem Moment an nur noch eine Überanpassung an den Trainingsdatensatz statt. In diesem Fall können wir das Training vorzeitig abbrechen. . Wir reduzieren den Plot auf die ersten 100 Zeitschritte um den Abbruchpunkt bestmöglich zu bestimmen. . binary_error_train = np.zeros(100) binary_error_test = np.zeros(100) prediction_train = np.zeros(X_train.shape[0]) prediction_test = np.zeros(X_test.shape[0]) for i in range(1, 101): prediction_train += ada.stumps[i-1].predict(X_train) * ada.stump_weights[i-1] prediction_test += ada.stumps[i-1].predict(X_test) * ada.stump_weights[i-1] binary_error_train[i-1] = np.sum(np.sign(prediction_train) != y_train) / X_train.shape[0] binary_error_test[i-1] = np.sum(np.sign(prediction_test) != y_test) / X_test.shape[0] plt.plot(binary_error_train, linestyle = &quot;solid&quot;, label=r&quot;binary error train&quot;) plt.plot(binary_error_test, linestyle = &quot;dotted&quot;, label=&quot;binary error test&quot;) plt.legend() plt.show() . Die genaue Bestimmung des Abbruchzeitpunkts ist zumeist schwer zu bestimmen. Ziel ist es, den Trainingsfehler und den Testfehler zu minimieren. Zumeist steigt der Testfehler aber wieder an, während der Trainingsfehler weiter sinkt. Im folgenden Beispiel werden die Zeitunkte 20, 30, 40 und 450 miteinander verglichen. . Durch das Hinzufügen weiterer Klassifikatoren in das Ensemble können zunehmend mehr Trainingsdaten richtig klassifiziert werden. Hierfür steigt die Komplexität der Entscheidungsgrenze des Ensembles und nimmt langsam die Form der Datenverteilung an. Gegeben der folgenden 4 Bilder scheint 40 ein interessanter Abbruchpunkt zu sein. . fig, ax = plt.subplots(1, 4, figsize=(16, 4)) for i, t in enumerate([20, 30, 40, 450]): truncated_ada = ada.truncate_adaboost(t) plot_results(X_train, y_train, truncated_ada, ax=ax[i], size_factor=0.1, highlight_errors=False) ax[i].set_title(&quot;Iteration &quot; + str(t)) plt.tight_layout() plt.show() .",
            "url": "https://adockhorn.github.io/Boosting/fastpages/jupyter/boosting/implementation/2021/03/12/boosting.html",
            "relUrl": "/fastpages/jupyter/boosting/implementation/2021/03/12/boosting.html",
            "date": " • Mar 12, 2021"
        }
        
    
  

  
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://adockhorn.github.io/Boosting/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}